{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face detection and recognition inference pipeline\n",
    "\n",
    "The following example illustrates how to use the `facenet_pytorch` python package to perform face detection and recogition on an image dataset using an Inception Resnet V1 pretrained on the VGGFace2 dataset.\n",
    "\n",
    "The following Pytorch methods are included:\n",
    "* Datasets\n",
    "* Dataloaders\n",
    "* GPU/CPU processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mamba\\envs\\default\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1, training\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "workers = 0 if os.name == 'nt' else 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine if an nvidia GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define MTCNN module\n",
    "\n",
    "Default params shown for illustration, but not needed. Note that, since MTCNN is a collection of neural nets and other code, the device must be passed in the following way to enable copying of objects when needed internally.\n",
    "\n",
    "See `help(MTCNN)` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Inception Resnet V1 module\n",
    "\n",
    "Set classify=True for pretrained classifier. For this example, we will use the model to output embeddings/CNN features. Note that for inference, it is important to set the model to `eval` mode.\n",
    "\n",
    "See `help(InceptionResnetV1)` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(pretrained='casia-webface').eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a dataset and data loader\n",
    "\n",
    "We add the `idx_to_class` attribute to the dataset to enable easy recoding of label indices to identity names later one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "dataset = datasets.ImageFolder('../hard_images')\n",
    "dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()}\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn, num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Ariana_Grande',\n",
       " 1: 'Ayumi_Hamasaki',\n",
       " 2: 'Haruka_Imai',\n",
       " 3: 'Haruna_Kawaguchi',\n",
       " 4: 'Yui_Aragaki'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.idx_to_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perfom MTCNN facial detection\n",
    "\n",
    "Iterate through the DataLoader object and detect faces and associated detection probabilities for each. The `MTCNN` forward method returns images cropped to the detected face, if a face was detected. By default only a single detected face is returned - to have `MTCNN` return all detected faces, set `keep_all=True` when creating the MTCNN object above.\n",
    "\n",
    "To obtain bounding boxes rather than cropped face images, you can instead call the lower-level `mtcnn.detect()` function. See `help(mtcnn.detect)` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# only get one image per person\n",
    "# people = set()\n",
    "def crop_faces(loader, dataset):\n",
    "    aligned = []\n",
    "    names = []\n",
    "    for x, y in loader:\n",
    "        person = dataset.idx_to_class[y]\n",
    "        # if person in people:\n",
    "        #     continue\n",
    "        # people.add(person)\n",
    "        x_aligned, prob = mtcnn(x, return_prob=True)\n",
    "        if x_aligned is not None:\n",
    "            #print('Face detected with probability: {:8f}'.format(prob))\n",
    "            aligned.append(x_aligned)\n",
    "            names.append(person)\n",
    "        else:\n",
    "            # print path of images that are not detected\n",
    "            print(f\"person not detection {person}\")\n",
    "    return aligned, names\n",
    "\n",
    "def create_embeddings(loader, dataset):\n",
    "    aligned, names = crop_faces(loader, dataset)\n",
    "    # split up aligned images into batches\n",
    "    aligned = torch.stack(aligned)\n",
    "    aligned = torch.split(aligned, 32)\n",
    "    # pass each batch through resnet\n",
    "    embeddings = []\n",
    "    for batch in aligned:\n",
    "        embeddings.extend(resnet(batch.to(device)).detach().cpu())\n",
    "    # convert embeddings to numpy\n",
    "    embeddings= torch.stack(embeddings).numpy()\n",
    "    names = np.array(names)\n",
    "    return embeddings, names, aligned\n",
    "\n",
    "#embeddings, names, aligned = create_embeddings(loader, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save embeddings and names\n",
    "# np.save('embeddings_hard.npy', embeddings)\n",
    "# np.save('names_hard.npy', names)\n",
    "np.save('embeddings_all.npy', embeddings)\n",
    "np.save('names_all.npy', names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in embeddings\n",
    "embeddings = np.load('embeddings_hard.npy')\n",
    "names = np.load('names_hard.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images_data = \"../generated_images\"\n",
    "generated_images_dataset = datasets.ImageFolder(generated_images_data)\n",
    "generated_images_dataset.idx_to_class = {i:c for c, i in generated_images_dataset.class_to_idx.items()}\n",
    "generated_images_loader = DataLoader(generated_images_dataset, collate_fn=collate_fn, num_workers=workers)\n",
    "\n",
    "# create embeddings\n",
    "embeddings_gen, names_gen, aligned_gen = create_embeddings(loader=generated_images_loader, dataset=generated_images_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate image embeddings\n",
    "\n",
    "MTCNN will return images of faces all the same size, enabling easy batch processing with the Resnet recognition module. Here, since we only have a few images, we build a single batch and perform inference on it. \n",
    "\n",
    "For real datasets, code should be modified to control batch sizes being passed to the Resnet, particularly if being processed on a GPU. For repeated testing, it is best to separate face detection (using MTCNN) from embedding or classification (using InceptionResnetV1), as calculation of cropped faces or bounding boxes can then be performed a single time and detected faces saved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from verifier import FacialVerifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 66\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m aucs, max_f_scores, train_sizes\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# test_names = np.array(['n009117', 'n003357'])\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# mask = np.isin(names, test_names)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# test_embeddings = embeddings[mask]\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# test_names = names[mask]\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m aucs, max_f_scores, train_sizes \u001b[38;5;241m=\u001b[39m \u001b[43mplot_auc_train_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn003357\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# n000854 n003357 n009117 \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 44\u001b[0m, in \u001b[0;36mplot_auc_train_size\u001b[1;34m(embeddings, names, celeb_name)\u001b[0m\n\u001b[0;32m     42\u001b[0m train_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_size \u001b[38;5;129;01min\u001b[39;00m train_sizes:\n\u001b[1;32m---> 44\u001b[0m     auc, max_accuracy, max_f_score \u001b[38;5;241m=\u001b[39m \u001b[43mget_auc_for_train_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceleb_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     aucs\u001b[38;5;241m.\u001b[39mappend(auc)\n\u001b[0;32m     46\u001b[0m     max_accuracies\u001b[38;5;241m.\u001b[39mappend(max_accuracy)\n",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m, in \u001b[0;36mget_auc_for_train_size\u001b[1;34m(embeddings, names, celeb_name, train_size)\u001b[0m\n\u001b[0;32m     10\u001b[0m test_embeddings \u001b[38;5;241m=\u001b[39m embeddings[mask]\n\u001b[0;32m     11\u001b[0m test_names \u001b[38;5;241m=\u001b[39m names[mask]\n\u001b[1;32m---> 13\u001b[0m verifier \u001b[38;5;241m=\u001b[39m \u001b[43mFacialVerifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknown_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mknown_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m thresholds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m     15\u001b[0m fpr_list, tpr_list, accuracies, f_scores, _ \u001b[38;5;241m=\u001b[39m verifier\u001b[38;5;241m.\u001b[39mtest_thresholds(test_embeddings, true_labels\u001b[38;5;241m=\u001b[39mtest_names, name\u001b[38;5;241m=\u001b[39mceleb_name, thresholds\u001b[38;5;241m=\u001b[39mthresholds)\n",
      "File \u001b[1;32md:\\projects\\Gen-AI\\FaceAugment\\examples\\verifier.py:4\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, known_embeddings, optimal_threshold)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_curve, auc\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFacialVerifier\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, known_embeddings, optimal_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "def get_auc_for_train_size(embeddings, names, celeb_name, train_size):\n",
    "    celeb_embedding = embeddings[names == celeb_name]\n",
    "    known_embeddings = celeb_embedding[:train_size]\n",
    "    known_indices = np.where(np.isin(names, celeb_name))[0][:train_size]\n",
    "    # Create a boolean mask for the remaining indices\n",
    "    mask = np.ones(len(embeddings), dtype=bool)\n",
    "    mask[known_indices] = False\n",
    "    # Use the mask to filter test_embeddings and test_names\n",
    "    test_embeddings = embeddings[mask]\n",
    "    test_names = names[mask]\n",
    "    \n",
    "    verifier = FacialVerifier(known_embeddings=known_embeddings)\n",
    "    thresholds = np.arange(0, 2, 0.05)\n",
    "    fpr_list, tpr_list, accuracies, f_scores, _ = verifier.test_thresholds(test_embeddings, true_labels=test_names, name=celeb_name, thresholds=thresholds)\n",
    "\n",
    "    # remove nan values from f_scores\n",
    "    f_scores = [f for f in f_scores if not np.isnan(f)]\n",
    "    max_f_score = max(f_scores)\n",
    "    max_accuracy = max(accuracies)\n",
    "    auc = verifier.compute_auc(fpr_list, tpr_list)\n",
    "    return auc, max_accuracy, max_f_score\n",
    "\n",
    "def get_auc_for_gen_embeddings(generated_embeddings, embeddings, names, celeb_name, train_size=None):\n",
    "    if train_size == None:\n",
    "        train_size = len(generated_embeddings)\n",
    "    known_embeddings = generated_embeddings[:train_size]\n",
    "    verifier = FacialVerifier(known_embeddings=known_embeddings)\n",
    "    thresholds = np.arange(0, 2, 0.05)\n",
    "    fpr_list, tpr_list, accuracies, f_scores, _ = verifier.test_thresholds(embeddings, true_labels=names, name=celeb_name, thresholds=thresholds)\n",
    "    max_accuracy = max(accuracies)\n",
    "    # remove nan values from f_scores\n",
    "    f_scores = [f for f in f_scores if not np.isnan(f)]\n",
    "    max_f_score = max(f_scores)\n",
    "    auc = verifier.compute_auc(fpr_list, tpr_list)\n",
    "    return auc, max_accuracy, max_f_score\n",
    "\n",
    "def plot_auc_train_size(embeddings, names, celeb_name):\n",
    "    aucs = []\n",
    "    max_accuracies = []\n",
    "    max_f_scores = []\n",
    "    train_sizes = list(range(1, 40, 1))\n",
    "    for train_size in train_sizes:\n",
    "        auc, max_accuracy, max_f_score = get_auc_for_train_size(embeddings, names, celeb_name, train_size)\n",
    "        aucs.append(auc)\n",
    "        max_accuracies.append(max_accuracy)\n",
    "        max_f_scores.append(max_f_score)\n",
    "    plt.plot(train_sizes, aucs)\n",
    "    plt.xlabel('Train Size')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title('AUC vs Train Size')\n",
    "    plt.show()\n",
    "    # plot max accuracies color line green\n",
    "    plt.plot(train_sizes, max_accuracies, color='green')\n",
    "    plt.xlabel('Train Size')\n",
    "    plt.ylabel('Max F Score')\n",
    "    plt.title('Max F Score vs Train Size')\n",
    "    # color line green\n",
    "    plt.show()\n",
    "    return aucs, max_f_scores, train_sizes\n",
    "\n",
    "# test_names = np.array(['n009117', 'n003357'])\n",
    "# mask = np.isin(names, test_names)\n",
    "# test_embeddings = embeddings[mask]\n",
    "# test_names = names[mask]\n",
    "aucs, max_f_scores, train_sizes = plot_auc_train_size(embeddings, names, 'n003357') # n000854 n003357 n009117 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX AUC IS 0.8730935671064379, max_accuray 0.9204389574759945, max_fscore 0.763265306122449\n",
      "TESTING ON BASE IMAGES NOW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anshu\\AppData\\Local\\Temp\\ipykernel_13676\\2398799882.py:65: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = tp / (tp + fp)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test generated accuracy using real embeddings and certain number of generated embeddings as known embeddings plot auc over gen size\n",
    "def test_generated_accuracy(gen_embeddings, real_embeddings, embeddings, names, celeb_name):\n",
    "    aucs = []\n",
    "    max_accuracies = []\n",
    "    max_f_scores = []\n",
    "    gen_sizes = list(range(1, len(gen_embeddings), 1))\n",
    "    for gen_size in gen_sizes:\n",
    "        known_embeddings = np.concatenate((gen_embeddings[:gen_size], real_embeddings))\n",
    "        auc, max_accuracy, max_f_score = get_auc_for_gen_embeddings(known_embeddings, embeddings, names, celeb_name)\n",
    "        aucs.append(auc)\n",
    "        max_accuracies.append(max_accuracy)\n",
    "        max_f_scores.append(max_f_score)\n",
    "    plt.plot(gen_sizes, aucs)\n",
    "    plt.xlabel('Gen Size')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title('AUC vs Gen Size')\n",
    "    plt.show()\n",
    "    # plot max f_scores color line green\n",
    "    plt.plot(gen_sizes, max_f_scores, color='green')\n",
    "    plt.xlabel('Gen Size')\n",
    "    plt.ylabel('Max F Score')\n",
    "    plt.title('Max F Score vs Gen Size')\n",
    "\n",
    "    # color line green\n",
    "    plt.show()\n",
    "    return aucs, max_accuracies, gen_sizes\n",
    "\n",
    "gen_embeddings = embeddings_gen[(names_gen == '0008_source')]\n",
    "real_embeddings = embeddings_gen[(names == 'n003357')]\n",
    "aucs, max_accuracies, gen_sizes = test_generated_accuracy(gen_embeddings, real_embeddings, embeddings, names, 'n003357')\n",
    "# swapped_embeddings = embeddings_gen[(names_gen == '0008_source')]#[0].reshape(-1, 512)\n",
    "# swapped_embeddings.shape\n",
    "# auc, max_accuracy, max_f_score = get_auc_for_gen_embeddings(generated_embeddings=swapped_embeddings, embeddings=embeddings, names=names, celeb_name=\"n003357\")\n",
    "# print(f\"MAX AUC IS {auc}, max_accuray {max_accuracy}, max_fscore {max_f_score}\")\n",
    "# print(\"TESTING ON BASE IMAGES NOW\")\n",
    "# auc, max_accuracy = get_auc_for_train_size(embeddings, names, celeb_name=\"n003357\", train_size=1)\n",
    "# print(f\"BASE AUC IS {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def create_and_save_Verifier(embeddings, celeb_name, file_path, train_size=1):\n",
    "    celeb_embedding = embeddings[names == celeb_name]\n",
    "    known_embeddings = celeb_embedding[:train_size]\n",
    "    verifier = FacialVerifier(known_embeddings=known_embeddings)\n",
    "    thresholds = np.arange(0, 2, 0.05)\n",
    "    verifier.test_thresholds(embeddings, true_labels=names, name=celeb_name, thresholds=thresholds)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(verifier, f)\n",
    "create_and_save_Verifier(embeddings, 'n003357', 'verifier_Haruna_Kawaguchi.pkl', train_size=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
