{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face detection and recognition inference pipeline\n",
    "\n",
    "The following example illustrates how to use the `facenet_pytorch` python package to perform face detection and recogition on an image dataset using an Inception Resnet V1 pretrained on the VGGFace2 dataset.\n",
    "\n",
    "The following Pytorch methods are included:\n",
    "* Datasets\n",
    "* Dataloaders\n",
    "* GPU/CPU processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mamba\\envs\\default\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1, training\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "workers = 0 if os.name == 'nt' else 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine if an nvidia GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define MTCNN module\n",
    "\n",
    "Default params shown for illustration, but not needed. Note that, since MTCNN is a collection of neural nets and other code, the device must be passed in the following way to enable copying of objects when needed internally.\n",
    "\n",
    "See `help(MTCNN)` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Inception Resnet V1 module\n",
    "\n",
    "Set classify=True for pretrained classifier. For this example, we will use the model to output embeddings/CNN features. Note that for inference, it is important to set the model to `eval` mode.\n",
    "\n",
    "See `help(InceptionResnetV1)` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(pretrained='casia-webface').eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a dataset and data loader\n",
    "\n",
    "We add the `idx_to_class` attribute to the dataset to enable easy recoding of label indices to identity names later one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "dataset = datasets.ImageFolder('../hard_images')\n",
    "dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()}\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn, num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Ariana_Grande',\n",
       " 1: 'Ayumi_Hamasaki',\n",
       " 2: 'Haruka_Imai',\n",
       " 3: 'Haruna_Kawaguchi',\n",
       " 4: 'Yui_Aragaki'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.idx_to_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perfom MTCNN facial detection\n",
    "\n",
    "Iterate through the DataLoader object and detect faces and associated detection probabilities for each. The `MTCNN` forward method returns images cropped to the detected face, if a face was detected. By default only a single detected face is returned - to have `MTCNN` return all detected faces, set `keep_all=True` when creating the MTCNN object above.\n",
    "\n",
    "To obtain bounding boxes rather than cropped face images, you can instead call the lower-level `mtcnn.detect()` function. See `help(mtcnn.detect)` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# only get one image per person\n",
    "# people = set()\n",
    "def crop_faces(loader, dataset):\n",
    "    aligned = []\n",
    "    names = []\n",
    "    for x, y in loader:\n",
    "        person = dataset.idx_to_class[y]\n",
    "        # if person in people:\n",
    "        #     continue\n",
    "        # people.add(person)\n",
    "        x_aligned, prob = mtcnn(x, return_prob=True)\n",
    "        if x_aligned is not None:\n",
    "            #print('Face detected with probability: {:8f}'.format(prob))\n",
    "            aligned.append(x_aligned)\n",
    "            names.append(person)\n",
    "        else:\n",
    "            # print path of images that are not detected\n",
    "            print(f\"person not detection {person}\")\n",
    "    return aligned, names\n",
    "\n",
    "def create_embeddings(loader, dataset):\n",
    "    aligned, names = crop_faces(loader, dataset)\n",
    "    # split up aligned images into batches\n",
    "    aligned = torch.stack(aligned)\n",
    "    aligned = torch.split(aligned, 32)\n",
    "    # pass each batch through resnet\n",
    "    embeddings = []\n",
    "    for batch in aligned:\n",
    "        embeddings.extend(resnet(batch.to(device)).detach().cpu())\n",
    "    # convert embeddings to numpy\n",
    "    embeddings= torch.stack(embeddings).numpy()\n",
    "    names = np.array(names)\n",
    "    return embeddings, names, aligned\n",
    "\n",
    "#embeddings, names, aligned = create_embeddings(loader, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save embeddings and names\n",
    "# np.save('embeddings_hard.npy', embeddings)\n",
    "# np.save('names_hard.npy', names)\n",
    "np.save('embeddings_all.npy', embeddings)\n",
    "np.save('names_all.npy', names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in embeddings\n",
    "embeddings = np.load('embeddings_hard.npy')\n",
    "names = np.load('names_hard.npy')\n",
    "\n",
    "generated_images_data = \"../generated_images\"\n",
    "generated_images_dataset = datasets.ImageFolder(generated_images_data)\n",
    "generated_images_dataset.idx_to_class = {i:c for c, i in generated_images_dataset.class_to_idx.items()}\n",
    "generated_images_loader = DataLoader(generated_images_dataset, collate_fn=collate_fn, num_workers=workers)\n",
    "\n",
    "# create embeddings\n",
    "embeddings_gen, names_gen, aligned_gen = create_embeddings(loader=generated_images_loader, dataset=generated_images_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate image embeddings\n",
    "\n",
    "MTCNN will return images of faces all the same size, enabling easy batch processing with the Resnet recognition module. Here, since we only have a few images, we build a single batch and perform inference on it. \n",
    "\n",
    "For real datasets, code should be modified to control batch sizes being passed to the Resnet, particularly if being processed on a GPU. For repeated testing, it is best to separate face detection (using MTCNN) from embedding or classification (using InceptionResnetV1), as calculation of cropped faces or bounding boxes can then be performed a single time and detected faces saved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "class FacialVerifier:\n",
    "    def __init__(self, known_embeddings):\n",
    "        self.known_embeddings = np.array(known_embeddings).astype('float32')\n",
    "        self.index = faiss.IndexFlatL2(self.known_embeddings.shape[1])\n",
    "        self.index.add(self.known_embeddings)\n",
    "\n",
    "    def verify(self, unknown_embeddings, true_labels, name, thresholds):\n",
    "        distances = self._compute_distances(unknown_embeddings)\n",
    "        true_labels = np.array([1 if label == name else 0 for label in true_labels])\n",
    "\n",
    "        fpr_list = []\n",
    "        tpr_list = []\n",
    "        accuracies = []\n",
    "        fscores = []\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            predicted_labels = np.array(distances) <= threshold\n",
    "\n",
    "            tp = np.sum((predicted_labels == 1) & (true_labels == 1))\n",
    "            fp = np.sum((predicted_labels == 1) & (true_labels == 0))\n",
    "            tn = np.sum((predicted_labels == 0) & (true_labels == 0))\n",
    "            fn = np.sum((predicted_labels == 0) & (true_labels == 1))\n",
    "\n",
    "            fpr = fp / (fp + tn)\n",
    "            tpr = tp / (tp + fn)\n",
    "\n",
    "            fpr_list.append(fpr)\n",
    "            tpr_list.append(tpr)\n",
    "\n",
    "            accuracy = (tp + tn) / len(unknown_embeddings)\n",
    "            fscore = self.calculate_fscore(tp, fp, tn, fn)\n",
    "            accuracies.append(accuracy)\n",
    "            fscores.append(fscore)\n",
    "            #print(f\"Threshold: {threshold:.2f}, Accuracy: {accuracy:.2f}, FPR: {fpr:.2f}, TPR: {tpr:.2f} FSCORE: {fscore:.2f}\")\n",
    "\n",
    "        return fpr_list, tpr_list, accuracies, fscores\n",
    "\n",
    "    # def _compute_distances(self, unknown_embeddings):\n",
    "    #     unknown_embeddings = np.array(unknown_embeddings).astype('float32')\n",
    "    #     distances, _ = self.index.search(unknown_embeddings, 1)\n",
    "    #     return distances.flatten()\n",
    "    # takes weighted average distance between k nearest neighbors\n",
    "    def _compute_distances(self, unknown_embeddings, k=5):\n",
    "        k = min(k, len(self.known_embeddings))\n",
    "        unknown_embeddings = np.array(unknown_embeddings).astype('float32')\n",
    "        distances, indices = self.index.search(unknown_embeddings, k)\n",
    "        weights = 1.0 / (distances + 1e-6)  # Add a small constant to avoid division by zero\n",
    "        weighted_distances = np.sum(distances * weights, axis=1) / np.sum(weights, axis=1)\n",
    "        return weighted_distances\n",
    "\n",
    "    def _compute_accuracy(self, distances, true_labels, threshold):\n",
    "        predicted_labels = np.array(distances) <= threshold\n",
    "        accuracy = np.mean(predicted_labels == true_labels)\n",
    "        return accuracy\n",
    "\n",
    "    def compute_auc(self, fpr_list, tpr_list):\n",
    "        auc_value = np.trapz(tpr_list, fpr_list)\n",
    "        return auc_value\n",
    "    \n",
    "    def calculate_fscore(self, tp, fp, tn, fn):\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        fscore = 2 * (precision * recall) / (precision + recall)\n",
    "        return fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "def get_auc_for_train_size(embeddings, names, celeb_name, train_size):\n",
    "    celeb_embedding = embeddings[names == celeb_name]\n",
    "    known_embeddings = celeb_embedding[:train_size]\n",
    "    known_indices = np.where(np.isin(names, celeb_name))[0][:train_size]\n",
    "    # Create a boolean mask for the remaining indices\n",
    "    mask = np.ones(len(embeddings), dtype=bool)\n",
    "    mask[known_indices] = False\n",
    "    # Use the mask to filter test_embeddings and test_names\n",
    "    test_embeddings = embeddings[mask]\n",
    "    test_names = names[mask]\n",
    "    \n",
    "    verifier = FacialVerifier(known_embeddings=known_embeddings)\n",
    "    thresholds = np.arange(0, 2, 0.05)\n",
    "    fpr_list, tpr_list, accuracies = verifier.verify(test_embeddings, true_labels=test_names, name=celeb_name, thresholds=thresholds)\n",
    "    max_accuracy = max(accuracies)\n",
    "    auc = verifier.compute_auc(fpr_list, tpr_list)\n",
    "    return auc, max_accuracy\n",
    "\n",
    "def get_auc_for_gen_embeddings(generated_embeddings, embeddings, names, celeb_name, train_size=None):\n",
    "    if train_size == None:\n",
    "        train_size = len(generated_embeddings)\n",
    "    known_embeddings = generated_embeddings[:train_size]\n",
    "    verifier = FacialVerifier(known_embeddings=known_embeddings)\n",
    "    thresholds = np.arange(0, 2, 0.05)\n",
    "    fpr_list, tpr_list, accuracies, f_scores = verifier.verify(embeddings, true_labels=names, name=celeb_name, thresholds=thresholds)\n",
    "    max_accuracy = max(accuracies)\n",
    "    # remove nan values from f_scores\n",
    "    f_scores = [f for f in f_scores if not np.isnan(f)]\n",
    "    max_f_score = max(f_scores)\n",
    "    auc = verifier.compute_auc(fpr_list, tpr_list)\n",
    "    return auc, max_accuracy, max_f_score\n",
    "\n",
    "def plot_auc_train_size(embeddings, names, celeb_name):\n",
    "    aucs = []\n",
    "    max_accuracies = []\n",
    "    train_sizes = list(range(1, 40, 1))\n",
    "    for train_size in train_sizes:\n",
    "        auc, max_accuracy = get_auc_for_train_size(embeddings, names, celeb_name, train_size)\n",
    "        aucs.append(auc)\n",
    "        max_accuracies.append(max_accuracy)\n",
    "    plt.plot(train_sizes, aucs)\n",
    "    plt.xlabel('Train Size')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title('AUC vs Train Size')\n",
    "    plt.show()\n",
    "    # plot max accuracies\n",
    "    plt.plot(train_sizes, max_accuracies)\n",
    "    plt.xlabel('Train Size')\n",
    "    plt.ylabel('Max Accuracy')\n",
    "    plt.title('Max Accuracy vs Train Size')\n",
    "    plt.show()\n",
    "    return aucs, train_sizes\n",
    "\n",
    "# test_names = np.array(['n009117', 'n003357'])\n",
    "# mask = np.isin(names, test_names)\n",
    "# test_embeddings = embeddings[mask]\n",
    "# test_names = names[mask]\n",
    "# aucs, train_sizes = plot_auc_train_size(embeddings, names, 'Ariana_Grande') # n000854 n003357 n009117 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX AUC IS 0.8730935671064379, max_accuray 0.9204389574759945, max_fscore 0.763265306122449\n",
      "TESTING ON BASE IMAGES NOW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anshu\\AppData\\Local\\Temp\\ipykernel_13676\\2398799882.py:65: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = tp / (tp + fp)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# select embeddings with name 0008_01_swapped_small\n",
    "swapped_embeddings = embeddings_gen[(names_gen == '0008_source')]#[0].reshape(-1, 512)\n",
    "swapped_embeddings.shape\n",
    "auc, max_accuracy, max_f_score = get_auc_for_gen_embeddings(generated_embeddings=swapped_embeddings, embeddings=embeddings, names=names, celeb_name=\"n003357\")\n",
    "print(f\"MAX AUC IS {auc}, max_accuray {max_accuracy}, max_fscore {max_f_score}\")\n",
    "print(\"TESTING ON BASE IMAGES NOW\")\n",
    "# auc, max_accuracy = get_auc_for_train_size(embeddings, names, celeb_name=\"n003357\", train_size=1)\n",
    "# print(f\"BASE AUC IS {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8109376549945443 0.8868312757201646\n"
     ]
    }
   ],
   "source": [
    "print(auc, max_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
